{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f11faf69",
   "metadata": {},
   "source": [
    "# Проект для «Викишоп»"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee807dd1",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Постройте модель со значением метрики качества *F1* не меньше 0.75. \n",
    "\n",
    "**Инструкция по выполнению проекта**\n",
    "\n",
    "1. Загрузите и подготовьте данные.\n",
    "2. Обучите разные модели. \n",
    "3. Сделайте выводы.\n",
    "\n",
    "Для выполнения проекта применять *BERT* необязательно, но вы можете попробовать.\n",
    "\n",
    "**Описание данных**\n",
    "\n",
    "Данные находятся в файле `toxic_comments.csv`. Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2c72ac",
   "metadata": {},
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0860d6a4",
   "metadata": {},
   "source": [
    "Откроем файл с данными и изучим их, для этого подключим библиотеку `pandas` помимо неё подключим еще остальные библиотеки, которые нам пригодятся. Для того чтобы прочитать данные из датасета воспользуемся методом `read_csv`, для получения общей информации о датасете воспользуемся методом `info`. Но для начала установим недостающие библиотеки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6f6fea9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymystem3 in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (0.2.0)\n",
      "Requirement already satisfied: requests in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from pymystem3) (2.28.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from requests->pymystem3) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from requests->pymystem3) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from requests->pymystem3) (2022.9.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from requests->pymystem3) (3.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pymystem3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e610b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: catboost in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (1.1.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from catboost) (3.5.2)\n",
      "Requirement already satisfied: graphviz in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from catboost) (0.20.1)\n",
      "Requirement already satisfied: pandas>=0.24.0 in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from catboost) (1.4.4)\n",
      "Requirement already satisfied: plotly in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from catboost) (5.9.0)\n",
      "Requirement already satisfied: six in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from catboost) (1.16.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from catboost) (1.9.1)\n",
      "Requirement already satisfied: numpy>=1.16.0 in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from catboost) (1.21.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from pandas>=0.24.0->catboost) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from pandas>=0.24.0->catboost) (2022.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (9.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (3.0.9)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (1.4.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (21.3)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (4.25.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from plotly->catboost) (8.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91ddec2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: swifter in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (1.3.4)\n",
      "Requirement already satisfied: parso>0.4.0 in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from swifter) (0.8.3)\n",
      "Requirement already satisfied: bleach>=3.1.1 in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from swifter) (4.1.0)\n",
      "Requirement already satisfied: pandas>=1.0.0 in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from swifter) (1.4.4)\n",
      "Requirement already satisfied: dask[dataframe]>=2.10.0 in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from swifter) (2022.7.0)\n",
      "Requirement already satisfied: psutil>=5.6.6 in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from swifter) (5.9.0)\n",
      "Requirement already satisfied: ipywidgets>=7.0.0 in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from swifter) (7.6.5)\n",
      "Requirement already satisfied: tqdm>=4.33.0 in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from swifter) (4.64.1)\n",
      "Requirement already satisfied: cloudpickle>=0.2.2 in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from swifter) (2.0.0)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from bleach>=3.1.1->swifter) (1.16.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from bleach>=3.1.1->swifter) (21.3)\n",
      "Requirement already satisfied: webencodings in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from bleach>=3.1.1->swifter) (0.5.1)\n",
      "Requirement already satisfied: toolz>=0.8.2 in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from dask[dataframe]>=2.10.0->swifter) (0.11.2)\n",
      "Requirement already satisfied: fsspec>=0.6.0 in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from dask[dataframe]>=2.10.0->swifter) (2022.7.1)\n",
      "Requirement already satisfied: partd>=0.3.10 in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from dask[dataframe]>=2.10.0->swifter) (1.2.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from dask[dataframe]>=2.10.0->swifter) (6.0)\n",
      "Requirement already satisfied: numpy>=1.18 in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from dask[dataframe]>=2.10.0->swifter) (1.21.5)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from ipywidgets>=7.0.0->swifter) (6.15.2)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from ipywidgets>=7.0.0->swifter) (5.1.1)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from ipywidgets>=7.0.0->swifter) (1.0.0)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from ipywidgets>=7.0.0->swifter) (0.2.0)\n",
      "Requirement already satisfied: ipython>=4.0.0 in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from ipywidgets>=7.0.0->swifter) (7.31.1)\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from ipywidgets>=7.0.0->swifter) (3.5.2)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from ipywidgets>=7.0.0->swifter) (5.5.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from pandas>=1.0.0->swifter) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from pandas>=1.0.0->swifter) (2.8.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from tqdm>=4.33.0->swifter) (0.4.5)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0->swifter) (7.3.4)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0->swifter) (0.1.6)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0->swifter) (1.5.5)\n",
      "Requirement already satisfied: pyzmq>=17 in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0->swifter) (23.2.0)\n",
      "Requirement already satisfied: tornado>=6.1 in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0->swifter) (6.1)\n",
      "Requirement already satisfied: debugpy>=1.0 in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0->swifter) (1.5.1)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->swifter) (0.7.5)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->swifter) (0.18.1)\n",
      "Requirement already satisfied: pygments in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->swifter) (2.11.2)\n",
      "Requirement already satisfied: backcall in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->swifter) (0.2.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->swifter) (63.4.1)\n",
      "Requirement already satisfied: decorator in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->swifter) (5.1.1)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->swifter) (3.0.20)\n",
      "Requirement already satisfied: jsonschema>=2.6 in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from nbformat>=4.2.0->ipywidgets>=7.0.0->swifter) (4.16.0)\n",
      "Requirement already satisfied: jupyter_core in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from nbformat>=4.2.0->ipywidgets>=7.0.0->swifter) (4.11.1)\n",
      "Requirement already satisfied: fastjsonschema in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from nbformat>=4.2.0->ipywidgets>=7.0.0->swifter) (2.16.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from packaging->bleach>=3.1.1->swifter) (3.0.9)\n",
      "Requirement already satisfied: locket in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from partd>=0.3.10->dask[dataframe]>=2.10.0->swifter) (1.0.0)\n",
      "Requirement already satisfied: notebook>=4.4.1 in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter) (6.4.12)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets>=7.0.0->swifter) (21.4.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets>=7.0.0->swifter) (0.18.0)\n",
      "Requirement already satisfied: entrypoints in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets>=7.0.0->swifter) (0.4)\n",
      "Requirement already satisfied: pywin32>=1.0 in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from jupyter_core->nbformat>=4.2.0->ipywidgets>=7.0.0->swifter) (302)\n",
      "Requirement already satisfied: terminado>=0.8.3 in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter) (0.13.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter) (3.1.2)\n",
      "Requirement already satisfied: prometheus-client in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter) (0.14.1)\n",
      "Requirement already satisfied: nbconvert>=5 in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter) (6.4.4)\n",
      "Requirement already satisfied: argon2-cffi in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter) (21.3.0)\n",
      "Requirement already satisfied: Send2Trash>=1.8.0 in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter) (1.8.0)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets>=7.0.0->swifter) (0.2.5)\n",
      "Requirement already satisfied: defusedxml in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter) (0.7.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter) (4.11.1)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter) (0.5.13)\n",
      "Requirement already satisfied: testpath in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter) (0.6.0)\n",
      "Requirement already satisfied: jupyterlab-pygments in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter) (0.1.2)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter) (1.5.0)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter) (0.8.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter) (2.0.1)\n",
      "Requirement already satisfied: pywinpty>=1.1.0 in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from terminado>=0.8.3->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter) (2.0.2)\n",
      "Requirement already satisfied: argon2-cffi-bindings in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter) (21.2.0)\n",
      "Requirement already satisfied: cffi>=1.0.1 in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter) (1.15.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter) (2.3.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\doctorlector\\anaconda3\\lib\\site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->swifter) (2.21)\n"
     ]
    }
   ],
   "source": [
    "!pip install swifter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aec7659d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from pymystem3 import Mystem\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import swifter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4c8bbe",
   "metadata": {},
   "source": [
    "Библиотеки импортировали, теперь инициализируем лемматизатор. Так как мы будем обучать модели машинного обучения на тексте, то необходимо убрать из датасета стоп-слова, которые не несут смысловой нагрузки, список стоп-слов тоже инициализируем. Также инициализируем необходимые данные для использования лемматизатора. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2872903b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DoctorLector\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\DoctorLector\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DoctorLector\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\DoctorLector\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\DoctorLector\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#LEMM_STEM = Mystem()\n",
    "LEMM_STEM = WordNetLemmatizer()\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('omw-1.4')\n",
    "STOP_WORDS = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1274243",
   "metadata": {},
   "source": [
    "Лемматизатор успешно инициализировался, можно переходить к изучению датасета:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6acfd7c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  toxic\n",
       "0           0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1           1  D'aww! He matches this background colour I'm s...      0\n",
       "2           2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3           3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4           4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    data = pd.read_csv('/datasets/toxic_comments.csv')\n",
    "except:\n",
    "    data = pd.read_csv('toxic_comments.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd05475b",
   "metadata": {},
   "source": [
    "Как видно на данных индекс был вынесен в отдельный столбец `Unnamed: 0`, удалим данный столбец из данных, так как он лишний и непонятно в каком порядке там указаны индексы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e2703ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159287</th>\n",
       "      <td>\":::::And for the second time of asking, when ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159288</th>\n",
       "      <td>You should be ashamed of yourself \\n\\nThat is ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159289</th>\n",
       "      <td>Spitzer \\n\\nUmm, theres no actual article for ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159290</th>\n",
       "      <td>And it looks like it was actually you who put ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159291</th>\n",
       "      <td>\"\\nAnd ... I really don't think you understand...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159292 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  toxic\n",
       "0       Explanation\\nWhy the edits made under my usern...      0\n",
       "1       D'aww! He matches this background colour I'm s...      0\n",
       "2       Hey man, I'm really not trying to edit war. It...      0\n",
       "3       \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4       You, sir, are my hero. Any chance you remember...      0\n",
       "...                                                   ...    ...\n",
       "159287  \":::::And for the second time of asking, when ...      0\n",
       "159288  You should be ashamed of yourself \\n\\nThat is ...      0\n",
       "159289  Spitzer \\n\\nUmm, theres no actual article for ...      0\n",
       "159290  And it looks like it was actually you who put ...      0\n",
       "159291  \"\\nAnd ... I really don't think you understand...      0\n",
       "\n",
       "[159292 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.drop(columns=data.columns[0], axis=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4113c8fd",
   "metadata": {},
   "source": [
    "Лишний столбец удалили, посмотрим теперь информацию о датасете:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fab1156d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159292 entries, 0 to 159291\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    159292 non-null  object\n",
      " 1   toxic   159292 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fa5a97",
   "metadata": {},
   "source": [
    "В датасете всего два столбца, с текстовыми комментариями и признаком о том является комментарий токсичным или нет. Также по данным видно, что пропусков в данных нет, проверим нет ли явных дубликатов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94cfadff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febafdcd",
   "metadata": {},
   "source": [
    "Явных дубликатов нет, проверим теперь есть ли неявные дубликаты в столбце `text`, возможно одинаковые комментарии указали со словами в разных регистрах. Переведем все комментари в нижний регистр:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bf6afa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['text'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe8a052",
   "metadata": {},
   "source": [
    "Посчитам теперь количество дубликатов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c6e2a54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959ef0b2",
   "metadata": {},
   "source": [
    "Получается у нас в датасете **45** одинаковых комментариев, удалим их:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf44953b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7522d2fe",
   "metadata": {},
   "source": [
    "Проверим количество дубликатов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75276d4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82972bff",
   "metadata": {},
   "source": [
    "Дубликатов больше не осталось, перейдем теперь непосредственно к обработке текста, перед тем как преобразовывать тексты в векторы и обучать модели машинного обучения. Нам необходимо преоразовать все слова в комментариях в начальную форму, то есть получить их лемму, а также избавиться от всех лишних символов, кроме пробелов, так как пробелы разделяют слова между собой. Напишем отдельные функции, которые будет выполнять указанные действия, для получения лемм ~~будем использовать бибилиотеку `pymystem3`~~ будем использовать лемматизатор  `WordNetLemmatizer` из библиотеки `nltk`, для очистки текста `регулярные выражения`. Лемматизатор будем использовать с POS-тегом, чтобы он находил лемму с учетом контекста:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba2ff306",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_text(text):\n",
    "    '''\n",
    "    Функция удаляет из текста лишние символы\n",
    "    '''\n",
    "    new_text = re.sub(r'[^a-zA-Z ]', ' ', text)\n",
    "    new_text = \" \".join(new_text.split())\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e260edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    '''\n",
    "    Функция возвращает POS-тэг по переданному слову\n",
    "    '''\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\n",
    "        \"J\": wordnet.ADJ,\n",
    "        \"N\": wordnet.NOUN,\n",
    "        \"V\": wordnet.VERB,\n",
    "        \"R\": wordnet.ADV\n",
    "    }\n",
    "    \n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d673780",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(text):\n",
    "    '''\n",
    "    Функция преобразует текст, возращает леммы слов по переданному тексту\n",
    "    '''\n",
    "    #lemm_list = LEMM_STEM.lemmatize(text)\n",
    "    lemm_list = [LEMM_STEM.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(text) if w not in STOP_WORDS]\n",
    "    lemm_text = \" \".join(lemm_list).rstrip()\n",
    "        \n",
    "    return lemm_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961bf7c4",
   "metadata": {},
   "source": [
    "Функции написали, применим их теперь к комментариям в датасете, запишем преобразованный текст в отдельный столбец:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4b2e00d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>text_clear</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>explanation\\nwhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>explanation why the edits made under my userna...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d'aww! he matches this background colour i'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>d aww he matches this background colour i m se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hey man, i'm really not trying to edit war. it...</td>\n",
       "      <td>0</td>\n",
       "      <td>hey man i m really not trying to edit war it s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nmore\\ni can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>more i can t make any real suggestions on impr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>you, sir, are my hero. any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>you sir are my hero any chance you remember wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159287</th>\n",
       "      <td>\":::::and for the second time of asking, when ...</td>\n",
       "      <td>0</td>\n",
       "      <td>and for the second time of asking when your vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159288</th>\n",
       "      <td>you should be ashamed of yourself \\n\\nthat is ...</td>\n",
       "      <td>0</td>\n",
       "      <td>you should be ashamed of yourself that is a ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159289</th>\n",
       "      <td>spitzer \\n\\numm, theres no actual article for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>spitzer umm theres no actual article for prost...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159290</th>\n",
       "      <td>and it looks like it was actually you who put ...</td>\n",
       "      <td>0</td>\n",
       "      <td>and it looks like it was actually you who put ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159291</th>\n",
       "      <td>\"\\nand ... i really don't think you understand...</td>\n",
       "      <td>0</td>\n",
       "      <td>and i really don t think you understand i came...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159247 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  toxic  \\\n",
       "0       explanation\\nwhy the edits made under my usern...      0   \n",
       "1       d'aww! he matches this background colour i'm s...      0   \n",
       "2       hey man, i'm really not trying to edit war. it...      0   \n",
       "3       \"\\nmore\\ni can't make any real suggestions on ...      0   \n",
       "4       you, sir, are my hero. any chance you remember...      0   \n",
       "...                                                   ...    ...   \n",
       "159287  \":::::and for the second time of asking, when ...      0   \n",
       "159288  you should be ashamed of yourself \\n\\nthat is ...      0   \n",
       "159289  spitzer \\n\\numm, theres no actual article for ...      0   \n",
       "159290  and it looks like it was actually you who put ...      0   \n",
       "159291  \"\\nand ... i really don't think you understand...      0   \n",
       "\n",
       "                                               text_clear  \n",
       "0       explanation why the edits made under my userna...  \n",
       "1       d aww he matches this background colour i m se...  \n",
       "2       hey man i m really not trying to edit war it s...  \n",
       "3       more i can t make any real suggestions on impr...  \n",
       "4       you sir are my hero any chance you remember wh...  \n",
       "...                                                   ...  \n",
       "159287  and for the second time of asking when your vi...  \n",
       "159288  you should be ashamed of yourself that is a ho...  \n",
       "159289  spitzer umm theres no actual article for prost...  \n",
       "159290  and it looks like it was actually you who put ...  \n",
       "159291  and i really don t think you understand i came...  \n",
       "\n",
       "[159247 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text_clear'] = data['text'].apply(lambda text_comment: clear_text(text_comment))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "74317bc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b97be641aea4a4ba2452edb61f510d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/159247 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>text_clear</th>\n",
       "      <th>text_lemm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>explanation\\nwhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>explanation why the edits made under my userna...</td>\n",
       "      <td>explanation edits make username hardcore metal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d'aww! he matches this background colour i'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>d aww he matches this background colour i m se...</td>\n",
       "      <td>aww match background colour seemingly stuck th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hey man, i'm really not trying to edit war. it...</td>\n",
       "      <td>0</td>\n",
       "      <td>hey man i m really not trying to edit war it s...</td>\n",
       "      <td>hey man really try edit war guy constantly rem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nmore\\ni can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>more i can t make any real suggestions on impr...</td>\n",
       "      <td>make real suggestion improvement wonder sectio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>you, sir, are my hero. any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>you sir are my hero any chance you remember wh...</td>\n",
       "      <td>sir hero chance remember page</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159287</th>\n",
       "      <td>\":::::and for the second time of asking, when ...</td>\n",
       "      <td>0</td>\n",
       "      <td>and for the second time of asking when your vi...</td>\n",
       "      <td>second time ask view completely contradicts co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159288</th>\n",
       "      <td>you should be ashamed of yourself \\n\\nthat is ...</td>\n",
       "      <td>0</td>\n",
       "      <td>you should be ashamed of yourself that is a ho...</td>\n",
       "      <td>ashamed horrible thing put talk page</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159289</th>\n",
       "      <td>spitzer \\n\\numm, theres no actual article for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>spitzer umm theres no actual article for prost...</td>\n",
       "      <td>spitzer umm there actual article prostitution ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159290</th>\n",
       "      <td>and it looks like it was actually you who put ...</td>\n",
       "      <td>0</td>\n",
       "      <td>and it looks like it was actually you who put ...</td>\n",
       "      <td>look like actually put speedy first version de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159291</th>\n",
       "      <td>\"\\nand ... i really don't think you understand...</td>\n",
       "      <td>0</td>\n",
       "      <td>and i really don t think you understand i came...</td>\n",
       "      <td>really think understand come idea bad right aw...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159247 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  toxic  \\\n",
       "0       explanation\\nwhy the edits made under my usern...      0   \n",
       "1       d'aww! he matches this background colour i'm s...      0   \n",
       "2       hey man, i'm really not trying to edit war. it...      0   \n",
       "3       \"\\nmore\\ni can't make any real suggestions on ...      0   \n",
       "4       you, sir, are my hero. any chance you remember...      0   \n",
       "...                                                   ...    ...   \n",
       "159287  \":::::and for the second time of asking, when ...      0   \n",
       "159288  you should be ashamed of yourself \\n\\nthat is ...      0   \n",
       "159289  spitzer \\n\\numm, theres no actual article for ...      0   \n",
       "159290  and it looks like it was actually you who put ...      0   \n",
       "159291  \"\\nand ... i really don't think you understand...      0   \n",
       "\n",
       "                                               text_clear  \\\n",
       "0       explanation why the edits made under my userna...   \n",
       "1       d aww he matches this background colour i m se...   \n",
       "2       hey man i m really not trying to edit war it s...   \n",
       "3       more i can t make any real suggestions on impr...   \n",
       "4       you sir are my hero any chance you remember wh...   \n",
       "...                                                   ...   \n",
       "159287  and for the second time of asking when your vi...   \n",
       "159288  you should be ashamed of yourself that is a ho...   \n",
       "159289  spitzer umm theres no actual article for prost...   \n",
       "159290  and it looks like it was actually you who put ...   \n",
       "159291  and i really don t think you understand i came...   \n",
       "\n",
       "                                                text_lemm  \n",
       "0       explanation edits make username hardcore metal...  \n",
       "1       aww match background colour seemingly stuck th...  \n",
       "2       hey man really try edit war guy constantly rem...  \n",
       "3       make real suggestion improvement wonder sectio...  \n",
       "4                           sir hero chance remember page  \n",
       "...                                                   ...  \n",
       "159287  second time ask view completely contradicts co...  \n",
       "159288               ashamed horrible thing put talk page  \n",
       "159289  spitzer umm there actual article prostitution ...  \n",
       "159290  look like actually put speedy first version de...  \n",
       "159291  really think understand come idea bad right aw...  \n",
       "\n",
       "[159247 rows x 4 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text_lemm'] = data['text_clear'].swifter.progress_bar(True).apply(lambda text_comment: lemmatize(text_comment))\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8163eb52",
   "metadata": {},
   "source": [
    "Текст успешно преобразовали, можем переходить к обучению моделей."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacf58b7",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35117046",
   "metadata": {},
   "source": [
    "У нас в датасете сейчас 4 столбца, оставим только два столбца\n",
    "- `text_lemm` — преобразованный и очищенный текст, будем этот столбец использовать в качестве признака;\n",
    "- `toxic` — флаг о том, токсичный указанный комментарий или нет, этот столбец будем использовать в качестве целевого признака.\n",
    "\n",
    "Удалим лишние данные из признаков оставив только выше указанные столбцы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b65f45a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_features = data.drop(['text', 'text_clear'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de7f8ea",
   "metadata": {},
   "source": [
    "Лишние признаки удалили, теперь разделим признаки на обучающую и тестовую выборки, обучающую выборку возьмем равную **80%**, тестовую равную **20%**. Берем такое разделение, так как планируем для поиска моделей использовать кросс-валидацию и `GridSearchCV`, указанные методы сами внутри разделяют переданную выборку на обучающую и валидационную."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d4dd89ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(127397, 2)\n",
      "(31850, 2)\n"
     ]
    }
   ],
   "source": [
    "data_train, data_test = train_test_split(data_features, test_size=0.2, random_state=12345)\n",
    "print(data_train.shape)\n",
    "print(data_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be725a2",
   "metadata": {},
   "source": [
    "Как видно по размерности выборки разделились корректно, вынесем теперь признаки и целевой признак в отдельные переменные у обучающей и тестовой выборки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1969f88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_features = data_train.drop(['toxic'], axis=1)\n",
    "data_train_target = data_train['toxic']\n",
    "\n",
    "data_test_features = data_test.drop(['toxic'], axis=1)\n",
    "data_test_target = data_test['toxic']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3b8c7a",
   "metadata": {},
   "source": [
    "Так как мы работаем с текстом, нам необходимо преобразовать этот текст в читаем вид для моделей машинного обучения, преобазуем текст в векторы с помощью *TF-IDF*, который учитывает важность слова в контексте документа для этого воспользуемся классов `TfidfVectorizer` из библиотеки `sklearn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "373e88e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_tf_idf = TfidfVectorizer(stop_words=STOP_WORDS)\n",
    "data_train_features_tf_idf = count_tf_idf.fit_transform(data_train_features['text_lemm'])\n",
    "data_test_features_tf_idf = count_tf_idf.transform(data_test_features['text_lemm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c81044a",
   "metadata": {},
   "source": [
    "Текст успешно преобразовался в векторы, теперь можно переходить к обучению моделей. Нам нужно определить по тексту указан положительный или негативный комментарий, то есть нужно решить задачу классификации, тогда для решения задачи будет использовать следующие модели — **Дерево решений**, **Случайный лес**, **Логистическая регрессия**. Для проверки моделей будем использовать метрику *F1*. Для поиска наилучшей модели сначала воспользуемся кросс-валидацией:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b4cd1fac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Кросс-валидация Дерево решений 0.7014340576052088\n",
      "F1 Кросс-валидация Случайного леса 0.6762413603810483\n",
      "F1 Кросс-валидация Логистической регрессии 0.7070644218247765\n"
     ]
    }
   ],
   "source": [
    "model_tree = DecisionTreeClassifier()\n",
    "model_forest = RandomForestClassifier(n_jobs=-1)\n",
    "model_clf = LogisticRegression(max_iter=1000)\n",
    "\n",
    "scores_tree = cross_val_score(model_tree, data_train_features_tf_idf, data_train_target, scoring='f1', cv=3, n_jobs=-1)\n",
    "scores_forest = cross_val_score(model_forest, data_train_features_tf_idf, data_train_target, scoring='f1', cv=3, n_jobs=-1)\n",
    "scores_clf = cross_val_score(model_clf, data_train_features_tf_idf, data_train_target, scoring='f1', cv=3, n_jobs=-1)\n",
    "\n",
    "print('F1 Кросс-валидация Дерево решений', abs(scores_tree.mean()))\n",
    "print('F1 Кросс-валидация Случайного леса', abs(scores_forest.mean()))\n",
    "print('F1 Кросс-валидация Логистической регрессии', abs(scores_clf.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fd1223",
   "metadata": {},
   "source": [
    "По заданию нам необходимо, чтобы значение метрики *F1* было не меньше **0.75**, с помощью кросс-валидации мы получили следующие резудьтаты по моделям:\n",
    "- **Дерево решений** — *0.701*;\n",
    "- **Случайный лес** — *0.676*;\n",
    "- **Логистическая регрессия** — *0.707*.\n",
    "\n",
    "Не одна модель не подходит под наши критерии, попробуем перебрать гиперпараметры у моделей **Дерево решений**, **Случайный лес** с помощью `GridSearchCV` может получиться подобрать модель с необходимым значением по метрике *F1*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "99aa2989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params {'max_depth': 60}\n",
      "Best score F1 0.7026886079754443\n"
     ]
    }
   ],
   "source": [
    "model_tree = DecisionTreeClassifier(random_state=12345)\n",
    "params_grid_tree = {\n",
    "    'max_depth': list(range(5, 61, 5))\n",
    "}\n",
    "grid_tree = GridSearchCV(model_tree, param_grid=params_grid_tree, scoring='f1', cv=3, n_jobs=-1)\n",
    "grid_tree.fit(data_train_features_tf_idf, data_train_target)\n",
    "print('Best params', grid_tree.best_params_)\n",
    "print('Best score F1', abs(grid_tree.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5980c628",
   "metadata": {},
   "source": [
    "При глубине деревьев равной **60** достигается наилучшее значение по метрике *F1* — **0.702**, но этого все равно недостаточно, по заданию значение метрики *F1* должно быть не меньше **0.75**. Попробуем теперь перебрать гиперпараметры у модели **Случайный лес**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7f00feb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params {'max_depth': 30, 'n_estimators': 10}\n",
      "Best score F1 0.029615720103462802\n"
     ]
    }
   ],
   "source": [
    "model_forest = RandomForestClassifier(random_state=12345, n_jobs=-1)\n",
    "params_grid_forest = {\n",
    "    'max_depth': list(range(5, 31, 5)),\n",
    "    'n_estimators': list(range(10, 51, 10))\n",
    "}\n",
    "grid_forest = GridSearchCV(model_forest, param_grid=params_grid_forest, scoring='f1', cv=3, n_jobs=-1)\n",
    "grid_forest.fit(data_train_features_tf_idf, data_train_target)\n",
    "print('Best params', grid_forest.best_params_)\n",
    "print('Best score F1', abs(grid_forest.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6372b1",
   "metadata": {},
   "source": [
    "При количестве деревьев равное **30** и глубине деревьев равной **10** у модели **Случайный лес** достигается наилучшее значение по метрике *F1* — **0.02** это слишком мало, наихудший результат по сравнению с другими моделями. Ни одна модель из выбранных ранее не показала нужное значение по метрике *F1*, воспользуемся моделью с градиентным спуском **CatBoost** для решения нашей задачи. Для перебора гиперпараметров для поиска наилучшей модели, будем использовать встроенный метод `grid_search`, чтобы метод `grid_search` при поиске модели использовал метрику *F1* укажем в атрибуте `eval_metric` значение *F1* при инициализации модели **CatBoost**. В модель **CatBoost** будем передавать текст не обработанный с помощью *TF-IDF*, так как **CatBoost** сам может преобразовать текстовые признаки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4c313acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_catboost = CatBoostClassifier(random_state=12345, verbose=25, eval_metric='F1', iterations=100, \n",
    "                                    text_features=['text_lemm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "88e13cf2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.0000000\ttest: 0.0000000\tbest: 0.0000000 (0)\ttotal: 166ms\tremaining: 16.5s\n",
      "25:\tlearn: 0.0000000\ttest: 0.0000000\tbest: 0.0000000 (0)\ttotal: 329ms\tremaining: 936ms\n",
      "50:\tlearn: 0.0000000\ttest: 0.0000000\tbest: 0.0000000 (0)\ttotal: 471ms\tremaining: 453ms\n",
      "75:\tlearn: 0.0000000\ttest: 0.0000000\tbest: 0.0000000 (0)\ttotal: 627ms\tremaining: 198ms\n",
      "99:\tlearn: 0.0000000\ttest: 0.0000000\tbest: 0.0000000 (0)\ttotal: 760ms\tremaining: 0us\n",
      "\n",
      "bestTest = 0\n",
      "bestIteration = 0\n",
      "\n",
      "0:\tloss: 0.0000000\tbest: 0.0000000 (0)\ttotal: 3.03s\tremaining: 9.09s\n",
      "0:\tlearn: 0.0000000\ttest: 0.0000000\tbest: 0.0000000 (0)\ttotal: 7.76ms\tremaining: 768ms\n",
      "25:\tlearn: 0.0000000\ttest: 0.0000000\tbest: 0.0000000 (0)\ttotal: 150ms\tremaining: 427ms\n",
      "50:\tlearn: 0.0000000\ttest: 0.0000000\tbest: 0.0000000 (0)\ttotal: 337ms\tremaining: 324ms\n",
      "75:\tlearn: 0.0000000\ttest: 0.0000000\tbest: 0.0000000 (0)\ttotal: 468ms\tremaining: 148ms\n",
      "99:\tlearn: 0.0000000\ttest: 0.0000000\tbest: 0.0000000 (0)\ttotal: 595ms\tremaining: 0us\n",
      "\n",
      "bestTest = 0\n",
      "bestIteration = 0\n",
      "\n",
      "1:\tloss: 0.0000000\tbest: 0.0000000 (0)\ttotal: 3.63s\tremaining: 3.63s\n",
      "0:\tlearn: 0.0000000\ttest: 0.0000000\tbest: 0.0000000 (0)\ttotal: 7.05ms\tremaining: 698ms\n",
      "25:\tlearn: 0.0000000\ttest: 0.0000000\tbest: 0.0000000 (0)\ttotal: 148ms\tremaining: 421ms\n",
      "50:\tlearn: 0.0000000\ttest: 0.0000000\tbest: 0.0000000 (0)\ttotal: 294ms\tremaining: 282ms\n",
      "75:\tlearn: 0.0000000\ttest: 0.0000000\tbest: 0.0000000 (0)\ttotal: 430ms\tremaining: 136ms\n",
      "99:\tlearn: 0.0000000\ttest: 0.0000000\tbest: 0.0000000 (0)\ttotal: 549ms\tremaining: 0us\n",
      "\n",
      "bestTest = 0\n",
      "bestIteration = 0\n",
      "\n",
      "2:\tloss: 0.0000000\tbest: 0.0000000 (0)\ttotal: 4.19s\tremaining: 1.4s\n",
      "0:\tlearn: 0.0000000\ttest: 0.0000000\tbest: 0.0000000 (0)\ttotal: 6.49ms\tremaining: 642ms\n",
      "25:\tlearn: 0.0000000\ttest: 0.0000000\tbest: 0.0000000 (0)\ttotal: 142ms\tremaining: 403ms\n",
      "50:\tlearn: 0.0000000\ttest: 0.0000000\tbest: 0.0000000 (0)\ttotal: 280ms\tremaining: 269ms\n",
      "75:\tlearn: 0.0000000\ttest: 0.0000000\tbest: 0.0000000 (0)\ttotal: 413ms\tremaining: 130ms\n",
      "99:\tlearn: 0.0000000\ttest: 0.0000000\tbest: 0.0000000 (0)\ttotal: 541ms\tremaining: 0us\n",
      "\n",
      "bestTest = 0\n",
      "bestIteration = 0\n",
      "\n",
      "3:\tloss: 0.0000000\tbest: 0.0000000 (0)\ttotal: 4.74s\tremaining: 0us\n",
      "Estimating final quality...\n",
      "Training on fold [0/3]\n",
      "0:\tlearn: 0.7012843\ttest: 0.7334760\tbest: 0.7334760 (0)\ttotal: 73.4ms\tremaining: 7.26s\n",
      "25:\tlearn: 0.7420619\ttest: 0.7574524\tbest: 0.7574524 (25)\ttotal: 1.72s\tremaining: 4.9s\n",
      "50:\tlearn: 0.7584122\ttest: 0.7615565\tbest: 0.7624396 (40)\ttotal: 3.31s\tremaining: 3.18s\n",
      "75:\tlearn: 0.7701187\ttest: 0.7645990\tbest: 0.7649196 (74)\ttotal: 4.95s\tremaining: 1.56s\n",
      "99:\tlearn: 0.7760355\ttest: 0.7654513\tbest: 0.7659464 (86)\ttotal: 6.46s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7659464355\n",
      "bestIteration = 86\n",
      "\n",
      "Training on fold [1/3]\n",
      "0:\tlearn: 0.6836767\ttest: 0.7255127\tbest: 0.7255127 (0)\ttotal: 46.2ms\tremaining: 4.57s\n",
      "25:\tlearn: 0.7342611\ttest: 0.7511911\tbest: 0.7521142 (24)\ttotal: 1.51s\tremaining: 4.31s\n",
      "50:\tlearn: 0.7552162\ttest: 0.7594074\tbest: 0.7594074 (50)\ttotal: 2.97s\tremaining: 2.85s\n",
      "75:\tlearn: 0.7684845\ttest: 0.7625130\tbest: 0.7625130 (75)\ttotal: 4.44s\tremaining: 1.4s\n",
      "99:\tlearn: 0.7750571\ttest: 0.7630103\tbest: 0.7639541 (98)\ttotal: 5.85s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7639540949\n",
      "bestIteration = 98\n",
      "\n",
      "Training on fold [2/3]\n",
      "0:\tlearn: 0.6966070\ttest: 0.7495933\tbest: 0.7495933 (0)\ttotal: 47.5ms\tremaining: 4.71s\n",
      "25:\tlearn: 0.7334472\ttest: 0.7593668\tbest: 0.7593668 (25)\ttotal: 1.55s\tremaining: 4.43s\n",
      "50:\tlearn: 0.7562241\ttest: 0.7700969\tbest: 0.7700969 (49)\ttotal: 3.08s\tremaining: 2.96s\n",
      "75:\tlearn: 0.7676993\ttest: 0.7693511\tbest: 0.7709366 (65)\ttotal: 4.61s\tremaining: 1.45s\n",
      "99:\tlearn: 0.7760668\ttest: 0.7709497\tbest: 0.7709526 (88)\ttotal: 6.04s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7709526288\n",
      "bestIteration = 88\n",
      "\n",
      "Best params {'depth': 5, 'iterations': 100, 'learning_rate': 0.35}\n",
      "Best score 0.7575859191669099\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'max_depth': [5, 10],\n",
    "    'learning_rate': [0.35, 0.45],\n",
    "    'iterations': [100]\n",
    "}\n",
    "grid_catboost = model_catboost.grid_search(params, X=data_train[['text_lemm']], y=data_train[['toxic']], cv=3)\n",
    "print('Best params', grid_catboost['params'])\n",
    "print('Best score', sum(grid_catboost['cv_results']['test-F1-mean']) / len(grid_catboost['cv_results']['test-F1-mean']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410f101e",
   "metadata": {},
   "source": [
    "При глубине дерева равной **5** и скорости обучения равной **0.35** модель показывает максимальный результат по метрике *F1* и он равен **0.75**, это наилучший результат по сравнению с другими моделями, также у этой модели подходящий показатель по метрике *F1* — не менее **0.75**. Будем использовать модель **CatBoost** для предсказаний на тестовой выборке, так как данная модель показала наилучший результат по сравнению с другими моделями. Обучим модель **CatBoost** с указанными гиперпараметрами и проверим её на тестовой выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1579d849",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.7081336\ttotal: 54.7ms\tremaining: 5.41s\n",
      "25:\tlearn: 0.7366767\ttotal: 1.7s\tremaining: 4.84s\n",
      "50:\tlearn: 0.7537963\ttotal: 3.36s\tremaining: 3.23s\n",
      "75:\tlearn: 0.7601027\ttotal: 4.93s\tremaining: 1.56s\n",
      "99:\tlearn: 0.7616096\ttotal: 6.32s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x167d7225f40>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_catboost = CatBoostClassifier(random_state=12345, verbose=25, eval_metric='F1', iterations=100, \n",
    "                                         text_features=['text_lemm'], depth=5, learning_rate=0.35)\n",
    "best_model_catboost.fit(data_train[['text_lemm']], data_train[['toxic']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263b7273",
   "metadata": {},
   "source": [
    "Модель **CatBoost** успешно обучилась, проверим её теперь на тестовой выборке с помощью метрики *F1*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b4afd95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 CatBoost на тестовой выборке: 0.7761702127659575\n"
     ]
    }
   ],
   "source": [
    "best_model_catboost_predictions = best_model_catboost.predict(data_test[['text_lemm']])\n",
    "print('F1 CatBoost на тестовой выборке:', f1_score(data_test_target, best_model_catboost_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34480b8f",
   "metadata": {},
   "source": [
    "На тестовой выборке у модели **CatBoost** значение по метрике *F1* равно **0.77**, это выше **0.75**, что говорит о том, что мы сделали правильный выбор в пользу данной модели. Проверим теперь модель на адекватность с помощью `DummyClassifier`, которая будет равновероятно распределять предсказанные значения классов, в нашем случае положительные и негативные комментарии:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e0d759d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 DummyClassifier на тестовой выборке 0.16910722720831367\n"
     ]
    }
   ],
   "source": [
    "model_dummy = DummyClassifier(strategy='uniform')\n",
    "model_dummy.fit(data_train_features_tf_idf, data_train_target)\n",
    "predictions_dummy = model_dummy.predict(data_test_features_tf_idf)\n",
    "result_dummy = f1_score(data_test_target, predictions_dummy)\n",
    "print('F1 DummyClassifier на тестовой выборке', result_dummy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dec082",
   "metadata": {},
   "source": [
    "Метрика *F1* на случайной модели имеет значение **0.17**, что значительно меньше у выбранной модели **CatBoost** и что говорит о том, что модель **CatBoost** делает адекватные предсказания."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d491e4",
   "metadata": {},
   "source": [
    "## Общий вывод"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db74ddff",
   "metadata": {},
   "source": [
    "В рамках задания нам необходимо было обучить модель, которая по тексту может классифицировать комментарий на позитивный и негативный. Для проверки моделей необходимо было использовать метрику *F1*. Изначально для обучения были выбраны следующие модели — **Дерево решений**, **Случайный лес**, **Логистическая регрессия**. Для обучения моделей использовалась кросс-валидация и `GridSearchCV`, были получены следующие результаты:\n",
    "\n",
    "Кросс-валидация с метрикой *F1*:\n",
    "\n",
    "- Дерево решений — **0.701**;\n",
    "- Случайный лес — **0.676**;\n",
    "- Логистическая регрессия — **0.707**.\n",
    "\n",
    "`GridSearchCV` с метрикой *F1* лучшие показатели:\n",
    "\n",
    "- Дерево решений — **0.702**;\n",
    "- Случайный лес — **0.02**.\n",
    "\n",
    "С помощью указанных выше методов не удалось найти модель со значением метрике *F1* не менее **0.75**, поэтому для обучения была выбрана модель с градиентным бустингом **CatBoost** с помощью метода `grid_search` удалось найти модель с подходящими гиперпараметрами и значением метрики *F1*. Данная модель была проверена на тестовой выборке и показала значение по метрике *F1* равное **0.77**, также данная модель была проверена на адекватность.\n",
    "\n",
    "На основании выше изложенного для классификации комментариев на позитивные и негативные необходимо использовать модель **CatBoost**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
